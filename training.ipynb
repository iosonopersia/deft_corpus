{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oBXU9TA8gI8"
      },
      "source": [
        "# Tesi di laurea di Simone Persiani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yixU8h1741gD"
      },
      "source": [
        "## Imports and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNMEyiE_43Y3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TdlQ9u47lv"
      },
      "source": [
        "Imports from PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UHFL-ZMx47Cy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch.nn import BCELoss, Module, Linear, Dropout\n",
        "from torch.optim import AdamW, SGD\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import WeightedRandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPptBT4PKdoh"
      },
      "source": [
        "Constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79bQx0Z0Kfhl",
        "outputId": "431780f8-65c2-4e59-9fb2-d0615806542e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda.\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}.\")\n",
        "\n",
        "EMBEDDINGS_SIZE = 768 # or 1024 for roberta_large\n",
        "MODEL_SAVE_PATH = \"./model.pt\"\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "config = {\n",
        "  \"learning_rate\": 1e-5,\n",
        "  \"epochs\": 4,  # Higher values (6, 8) usually lead to overfitting (based on my results)\n",
        "  \"hidden_layer_size\": 64,  # Higher values (128, 256) usually lead to overfitting (based on my results), lower values (32) to underfitting!\n",
        "  \"batch_size\": 16,\n",
        "  \"weight_decay\": 0.01,  # NEW\n",
        "  \"label_smoothing\": 0.05,  # NEW\n",
        "  \"dataset\": \"augmented\",  # \"preprocessed\" | [\"augmented\"  # NEW]\n",
        "  \"max_sequence_length\": 60  # lower value (30) decreases train/val accuracy (about -1%), higher values (90, 120) don't lead to any improvement\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck_M4QzaHjCD"
      },
      "source": [
        "Setting random seeds to obtain a deterministic behaviour:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w1yu2ym7HoEG"
      },
      "outputs": [],
      "source": [
        "def random_state(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "random_state(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uERRbKGxzCDD"
      },
      "source": [
        "## Download RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1NlmHhptiJJ4"
      },
      "outputs": [],
      "source": [
        "from transformers.optimization import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQl6KLAta25F"
      },
      "source": [
        "Download the model and the tokenizer.\n",
        "\n",
        "[This work](https://github.com/avramandrei/UPB-SemEval-2020-Task-6) showed that a fine-tuned RoBERTa model is the best-performing variant of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue_YKPCOaS7c",
        "outputId": "7deaba21-02e1-4b0c-8d96-d34cc2121087"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_tokenizer.add_tokens([\"<link>\", \"<equation>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQuxzzA4MDM"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f5GngRPp2mcH"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path, tokenizer):\n",
        "  df = pd.read_csv(dataset_path, sep=\"\\t\", header=0, encoding='utf-8',\n",
        "                   names=[\"SENTENCE\", \"HAS_DEF\"], usecols=[\"SENTENCE\", \"HAS_DEF\"],\n",
        "                   dtype={\"SENTENCE\": str, \"HAS_DEF\": np.uint8})\n",
        "\n",
        "  X, y = df[\"SENTENCE\"].tolist(), df[\"HAS_DEF\"].tolist()\n",
        "\n",
        "  encodings = tokenizer(X, add_special_tokens=True, max_length=config[\"max_sequence_length\"],\n",
        "                        padding=\"longest\", truncation=\"longest_first\",\n",
        "                        return_attention_mask=True, return_tensors=\"pt\")\n",
        "\n",
        "  X = encodings['input_ids'].to(dtype=torch.int32, device='cpu')\n",
        "  y = torch.tensor(y, dtype=torch.int64, device='cpu')\n",
        "  mask = encodings['attention_mask'].to(dtype=torch.uint8, device='cpu')\n",
        "\n",
        "  dataset = TensorDataset(X, y, mask)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "train_ds = load_dataset(f'./dataset/{config[\"dataset\"]}/train.tsv', roberta_tokenizer)\n",
        "val_ds   = load_dataset(f'./dataset/{config[\"dataset\"]}/dev.tsv',   roberta_tokenizer)\n",
        "test_ds  = load_dataset(f'./dataset/preprocessed/test.tsv',  roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4MytJ9eIfeF"
      },
      "source": [
        "## Dealing with an unbalanced dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVxJb5rMBmOf"
      },
      "source": [
        "Preparing a WeightedRandomSampler so that training batches will contain, _on average_, the same amount of positive and negative samples.\n",
        "\n",
        "**This should address the problem of the unbalanced DEFT dataset. In general, one would expect definitions to be a relatively-rare occurrence in a Natural Language text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oUn9XpDeRAIK"
      },
      "outputs": [],
      "source": [
        "#def getBalancingWeights(labels):  # NEW\n",
        "#    n_samples = labels.shape[0]\n",
        "#    def_samples = torch.sum(labels).item()\n",
        "#\n",
        "#    non_def_samples = n_samples - def_samples\n",
        "#\n",
        "#    class_weights = {0: def_samples / n_samples,\n",
        "#                    1: non_def_samples / n_samples}\n",
        "#    sample_weights = torch.tensor([class_weights[label.item()] for label in labels], dtype=torch.double, device='cpu')\n",
        "#\n",
        "#    return sample_weights, class_weights\n",
        "#\n",
        "#labels = train_ds.tensors[1]\n",
        "#sample_weights, class_weights = getBalancingWeights(labels)\n",
        "#g = torch.Generator()\n",
        "#g.manual_seed(RANDOM_SEED)\n",
        "#weighted_sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "#                                        num_samples=len(train_ds),\n",
        "#                                        replacement=True, generator=g)\n",
        "## Param replacement=True means that the same sample can be selected more than once inside a single batch!\n",
        "\n",
        "#train_loader = DataLoader(dataset=train_ds, batch_size=config[\"batch_size\"], sampler=weighted_sampler)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "val_loader   = DataLoader(dataset=val_ds, batch_size=config[\"batch_size\"], sampler=SequentialSampler(val_ds))\n",
        "test_loader  = DataLoader(dataset=test_ds, batch_size=config[\"batch_size\"], sampler=SequentialSampler(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Hujp7J30OD"
      },
      "source": [
        "**NEW** As an alternative, use weighted loss (by weighing more the errors done on the minority class. Just like the WeightedRandomSampler, this technique must be applied to the training set ONLY (no validation/test)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z0ksmwYRhtS6"
      },
      "outputs": [],
      "source": [
        "n_samples = len(train_ds)\n",
        "def_samples = torch.sum(train_ds.tensors[1]).item()\n",
        "\n",
        "non_def_samples = n_samples - def_samples\n",
        "\n",
        "class_weights = {0: def_samples / n_samples, 1: non_def_samples / n_samples}\n",
        "\n",
        "def getBATCHBalancingWeights(labels):  # NEW\n",
        "    sample_weights = torch.tensor([class_weights[label.item()] for label in labels], dtype=torch.double, device=DEVICE)\n",
        "    return sample_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FnCfQ5eIaJt"
      },
      "source": [
        "## Label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WTW80qKNRBGx"
      },
      "outputs": [],
      "source": [
        "def smooth_labels(labels, smoothing = 0.0):  # NEW\n",
        "    assert 0 <= smoothing < 1\n",
        "\n",
        "    confidence = 1.0 - smoothing\n",
        "    uniform_probability = 0.5\n",
        "\n",
        "    smoothed_true_label = confidence + smoothing * uniform_probability\n",
        "    smoothed_false_label = smoothing * uniform_probability\n",
        "\n",
        "    smoothed_labels = torch.tensor([smoothed_true_label if v == 1 else smoothed_false_label for v in labels], dtype=torch.float, device=DEVICE, requires_grad=False)\n",
        "\n",
        "    return smoothed_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sNSYhNzGj9"
      },
      "source": [
        "## Define the classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NAQxZB6OzKkl"
      },
      "outputs": [],
      "source": [
        "class RoBERTaWithMLP(Module):\n",
        "  \"\"\" See: https://github.com/avramandrei/UPB-SemEval-2020-Task-6/blob/77d92e9c386f270af6ed1db259d3ba6e8bde307b/task1/model.py#L49-L80 \"\"\"\n",
        "  \n",
        "  def __init__(self, lang_model, vocab_size, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.lang_model = lang_model\n",
        "    self.lang_model.resize_token_embeddings(vocab_size)\n",
        "\n",
        "    self.linear1 = Linear(input_size, hidden_size)\n",
        "    self.dropout1 = Dropout(0.8)\n",
        "    self.linear2 = Linear(hidden_size, hidden_size)\n",
        "    self.dropout2 = Dropout(0.8)\n",
        "    self.linear3 = Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    embeddings = self.lang_model(x, attention_mask=mask)[0]\n",
        "    embeddings = torch.mean(embeddings, dim=1)\n",
        "\n",
        "    output = self.dropout1(F.gelu(self.linear1(embeddings)))\n",
        "    output = self.dropout2(F.gelu(self.linear2(output)))\n",
        "    output = torch.sigmoid(self.linear3(output))\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SR48nBa0Bfs"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BJCNUdok_yQy"
      },
      "outputs": [],
      "source": [
        "def evaluate(model):\n",
        "  criterion = BCELoss()\n",
        "  loss, acc, f1 = (0,) * 3\n",
        "  with torch.no_grad():\n",
        "    for (val_x, val_y, mask) in val_loader:\n",
        "      # Move data to the device in use\n",
        "      val_x = val_x.to(DEVICE)\n",
        "      val_y = val_y.to(DEVICE)\n",
        "      mask = mask.to(DEVICE)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model.forward(val_x, mask)\n",
        "      output = torch.reshape(output, (-1,))\n",
        "\n",
        "      smoothed_labels = smooth_labels(val_y, config['label_smoothing'])\n",
        "      curr_loss = criterion(output, smoothed_labels)\n",
        "      # Don't apply weighted loss to validation set!  # NEW\n",
        "\n",
        "      # Performance evaluation\n",
        "      pred = torch.tensor([0 if x < 0.5 else 1 for x in output])\n",
        "      curr_acc = accuracy_score(val_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_prec = precision_score(val_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_rec = recall_score(val_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_f1 = f1_score(val_y.cpu(), pred.cpu()) * 100.0\n",
        "\n",
        "      loss += float(curr_loss.item())\n",
        "      acc += float(curr_acc)\n",
        "      #f1 += float(curr_f1)\n",
        "      # prec += curr_prec\n",
        "      # rec += curr_rec\n",
        "      \n",
        "    loss /= len(val_loader)\n",
        "    acc /= len(val_loader)\n",
        "    # f1 /= len(val_loader)\n",
        "    # prec /= len(val_loader)\n",
        "    # rec /= len(val_loader)\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcxtUQ8a0DEv",
        "outputId": "215fbb54-24e3-4e81-fdbd-9622fa94ce31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EPOCH 1] Accuracy score improved from 0 -> 85.8. Saving model... DONE!\n",
            "[EPOCH 2] Accuracy score improved from 85.8 -> 85.86. Saving model... DONE!\n",
            "[EPOCH 3] Accuracy score improved from 85.86 -> 86.23. Saving model... DONE!\n",
            "[EPOCH 4] Accuracy score improved from 86.23 -> 86.53. Saving model... DONE!\n"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "  vocab_size = len(roberta_tokenizer) # 50265 + 2\n",
        "\n",
        "  model = RoBERTaWithMLP(roberta_model,\n",
        "                         vocab_size,\n",
        "                         EMBEDDINGS_SIZE,\n",
        "                         config[\"hidden_layer_size\"]\n",
        "                         ).to(DEVICE)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "  #optimizer = SGD(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"], nesterov=True, momentum=0.9)\n",
        "\n",
        "  total_steps = len(train_loader) * config[\"epochs\"]\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "\n",
        "  criterion = BCELoss(reduction='none')  # NEW\n",
        "  best_acc = 0\n",
        "\n",
        "  for epoch in range(config[\"epochs\"]):\n",
        "    model.train()\n",
        "\n",
        "    loss, acc, f1, prec, rec = (0,) * 5\n",
        "\n",
        "    for i, (train_x, train_y, mask) in enumerate(train_loader):\n",
        "      # Move data to the device in use\n",
        "      train_x = train_x.to(DEVICE)\n",
        "      train_y = train_y.to(DEVICE)\n",
        "      mask = mask.to(DEVICE)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model.forward(train_x, mask)\n",
        "      output = torch.reshape(output, (-1,))\n",
        "\n",
        "      smoothed_labels = smooth_labels(train_y, config['label_smoothing'])\n",
        "      curr_loss = criterion(output, smoothed_labels)\n",
        "      weights = getBATCHBalancingWeights(train_y)  # NEW\n",
        "      curr_loss = torch.mean(weights*curr_loss)  # NEW\n",
        "\n",
        "      # Backward pass\n",
        "      optimizer.zero_grad()\n",
        "      curr_loss.backward()\n",
        "\n",
        "      # Parameters update\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # Performance evaluation\n",
        "      pred = torch.tensor([0 if x < 0.5 else 1 for x in output])\n",
        "      curr_acc = accuracy_score(train_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_f1 = f1_score(train_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_prec = precision_score(train_y.cpu(), pred.cpu()) * 100.0\n",
        "      # curr_rec = recall_score(train_y.cpu(), pred.cpu()) * 100.0\n",
        "\n",
        "      loss += float(curr_loss.item())\n",
        "      acc += float(curr_acc)\n",
        "      # f1 += curr_f1\n",
        "      # prec += curr_prec\n",
        "      # rec += curr_rec\n",
        "\n",
        "    model.eval()\n",
        "    loss, acc = evaluate(model)\n",
        "\n",
        "    if acc > best_acc:\n",
        "      print(f\"[EPOCH {epoch + 1}] Accuracy score improved from {round(best_acc, 2)} -> {round(acc, 2)}. Saving model...\", end=\"\")\n",
        "      best_acc = acc\n",
        "      torch.save(model, MODEL_SAVE_PATH)\n",
        "      print(\" DONE!\")\n",
        "    else:\n",
        "      print(f\"[EPOCH {epoch + 1}] Accuracy score didn't improved... best value is {round(best_acc, 2)} while current result is {round(acc, 2)}.\")\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "trained_model = train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pV8KSEDCVb2T"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0dvS5IAWdh9G",
        "uERRbKGxzCDD",
        "xWQuxzzA4MDM",
        "a4MytJ9eIfeF",
        "1FnCfQ5eIaJt",
        "X_sNSYhNzGj9"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "2fab139eab8d276e062bfdde454fcd48b84ed6bf64c5efaf64b2109faa450613"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
